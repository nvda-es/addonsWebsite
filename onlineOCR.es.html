
<h1>Descriptor de im&#xE1;genes en l&#xED;nea</h1>
<ul>
<li>Autor: Larry Wang</li>
<li>NVDA compatibility: from 2018.3 to 2020.2</li>
<li>Descargar <a href="https://addons.nvda-project.org/files/get.php?file=oid-dev">versión de desarrollo</a></li>
</ul>


<p>This addon aims at adding online image recognition engines to NVDA.  There
are two types of engines. OCR and image describer.  OCR extract text from
image.  Image describer describe visual features in image in text form.
Such as general description, color type landmarks and so on.  Internet
connection is required to use this addon, since image describe services are
provided by API endpoints on the Internet.  They are called engines in this
addon.  There are three types of engine for this addon.</p>

<ul>
<li>Online OCR engine</li>
<li>Online image describer engine</li>
<li>Windows 10 OCR engine (offline)</li>
</ul>


<p>You also need to choose the source of recognition image.</p>

<ul>
<li>Current navigator object</li>
<li>Current foreground window</li>
<li>The whole screen</li>
<li>Image data or file from clipboard</li>
<li>Image file pathname or image url from clipboard</li>
</ul>


<h3>Órdenes de teclado</h3>

<p>After choosing these types, you can start recognition with one gesture.
NVDA+Alt+P Perform recognize according to source and engine type setting,
Then read result. If pressed twice, open a virtual result document.</p>

<p>There are four additional gestures left unassigned. Please assign them
before using.  Cycle through different recognition engine types.  cycle
through different recognition source types.  Cancel current recognition This
gesture can be useful if you think you have waited for too long and want to
cancel.  Also sometimes you do not want to be disturbed by recognition
message because you need to review some messages arrived after recognition
start.</p>

<p>Show previous result in a virtual result document.  Though there is a
feature to copy result to clipboard. Character position information cannot
be preserved, so this gesture is added to solve this problem.</p>

<p>There are also four old gestures are left unassigned for users who prefer
gestures in previous versions.  It is recommended to use new gesture and
switch engine type according to your need.</p>

<p>Recognize current navigator object with online OCR engine Then read
result. If pressed twice, open a virtual result document.</p>

<p>Recognizes image in clipboard with online OCR engine. Then read result. If
pressed twice, open a virtual result document.</p>

<p>Recognize current navigator object Then read result. If pressed twice, open
a virtual result document.</p>

<p>Recognizes image in clipboard . Then read result. If pressed twice, open a
virtual result document.</p>

<h2>Configuración de motor</h2>

<p>Puedes elegir motores de reconocimiento y configurarlos en detalle en la
categoría <em>Online image describer</em> del diálogo de opciones de NVDA.</p>

<p>El autor de este complemento ha registrado cuentas con cuota de API gratuita
y ha configurado un servidor proxy en www.nvdacn.com para permitir que este
complemento sea más fácil de probar al principio. La cuota de pruebas está
limitada y puede ser cancelada por el proveedor de la API en cualquier
momento. Se recomienda encarecidamente que registres tu propia clave
siguiendo la guía de cada motor.</p>

<p>Los siguientes ajustes se aplican a todos los motores.</p>

<ul>
<li><p>Copy recognition result to the clipboard:
If enabled, recognition result text will be copied to clipboard after recognition.</p></li>
<li><p>Use browseable message for text result
If enabled, recognition result text will be shown in a popup window instead of speech or braille message.</p></li>
<li><p>Swap the effect of repeated gesture with none repeated ones:
By default, a virtual result document is shown only if you press the corresponding gesture twice, if you use that frequently you can enable this option so that you only need to press once to get a result viewer.</p></li>
<li><p>Enable more verbose logging for debug purposes:
Some logs are essential for debugging but affects performance and takes up a lot of space. Only turn this on if specifically instructed to by the addon author or an NVDA developer.</p></li>
<li><p>Proxy type:
Which type of proxy you are using. If you do not know what a proxy is just leave it as is.</p></li>
<li><p>Proxy address:
Full URL of your proxy. If you do not know what a proxy is just leave it as is.
If you choose to use proxy your proxy will be verified before saving , after verification, there will be a prompt to tell you result.
The following settings means the same in all engines, describe them here to save space.</p></li>
<li><p>API Access Type:
This controls how you get access to the corresponding API endpoints.
If you choose "Use public quota", you are using free quota in an account registered by addon author.
If you choose "Use your own API key", this addon will use quota from your own account.</p></li>
<li><p>APP ID, API key or API Secret Key:
If you want to use quota from your own account corresponding access tokens is required. Some engines only need API key.
Some engines require two tokens.
These are only valid if you choose "use your own API key" in API Access type.</p></li>
</ul>


<p>Note that the quality and accuracy of results are affected by many factors.</p>

<ul>
<li>Modelos y técnicas empleados por el proveedor del motor</li>
<li>Calidad de la imagen subida</li>
<li>Navegador de objetos oculto tras algo</li>
<li>Resolución de pantalla</li>
</ul>


<h2>Descripción de imágenes en línea</h2>

<p>Hay tres motores disponibles.</p>

<h3>Analizador de imágenes de Microsoft Azure</h3>

<p>Este motor extrae un rico conjunto de características visuales basadas en el
contenido de la imagen. Este motor está sólo en inglés. Si quieres que la
descripción se muestre en otros idiomas, puedes usar el descriptor de
imágenes de Microsoft Azure</p>

<p>Visual Features include: Adult - detects if the image is pornographic in
nature (depicts nudity or a sex act). Sexually suggestive content is also
detected.  Brands - detects various brands within an image, including the
approximate location. The Brands argument is only available in English.
Categories - categorizes image content according to a taxonomy defined in
documentation.  Color - determines the accent color, dominant color, and
whether an image is black&amp;white.  Description - describes the image content
with a complete sentence in supported languages.  Faces - detects if faces
are present. If present, generate coordinates, gender and age.  ImageType -
detects if image is clip art or a line drawing.  Objects - detects various
objects within an image, including the approximate location. The Objects
argument is only available in English.  Tags - tags the image with a
detailed list of words related to the image content.</p>

<p>Algunas características también proporcionan detalles adicionales:</p>

<p>Celebrities - identifies celebrities if detected in the image.  Landmarks -
identifies landmarks if detected in the image.</p>

<h3>Descriptor de imágenes de Microsoft Azure</h3>

<p>Este motor genera descripciones de imágenes en un lenguaje natural con
frases completas. La descripción se basa en una colección de etiquetas de
contenido, también devueltas por la operación. Se puede generar más de una
descripción para cada imagen. Las descripciones se ordenan según su
puntuación de confianza. Hay dos ajustes para este motor.</p>

<ul>
<li><p>Language
The language in which the service will return a description of the image. English by default.</p></li>
<li><p>Maximum Candidates
Maximum number of candidate descriptions to be returned. The default is 1.</p></li>
</ul>


<h2>OCR en línea</h2>

<p>Los motores en línea se apoyan en el uso y presencia de los siguientes
servicios.</p>

<p>https://www.nvdacn.com</p>

<p>https://ocr.space/ocrapi</p>

<p>https://azure.microsoft.com/en-us/services/cognitive-services/</p>

<p>http://ai.qq.com</p>

<p>http://ai.baidu.com</p>

<p>http://ai.sogou.com/</p>

<p>https://intl.cloud.tencent.com</p>

<h2>Motores</h2>

<p>Hay cinco motores disponibles.</p>

<h3>Tencent Cloud OCR</h3>

<p>This API is sponsored by <a href="https://intl.cloud.tencent.com">Tencent Cloud</a> and
<a href="http://www.siaa.org.cn">Aceessibility Research Association</a>, with a quota
of 15000 per day.</p>

<p>This engine support 19 languages.</p>

<p>Chinese-English mix Japanese Korean Spanish French German Portuguese
Vietnamese Malay Russian Italian Dutch Swedish Finnish Danish Norwegian
Hungarian Thai Latin</p>

<p>Here is the settings of this engine.</p>

<p>Idioma: idioma de reconocimiento del texto. Por defecto detectar
automáticamente.</p>

<h3>OCR space</h3>

<p>This one is a paid API with free quota provided by https://ocr.space
It supports 24 languages including
Arabic
Bulgarian
Chinese(Simplified)
Chinese(Traditional)
Croatian
Czech
Danish
Dutch
English
Finnish
French
German
Greek
Hungarian
Korean
Italian
Japanese
Polish
Portuguese
Russian
Slovenian
Spanish
Swedish
Turkish</p>

<p>Estos son los ajustes de este motor:</p>

<p>Idioma: idioma de reconocimiento del texto. Por defecto inglés.</p>

<p>Detectar orientación de la imagen: si se activa, la API gira automáticamente
la imagen correctamente.</p>

<p>Scale image for better quality If set to true, the API does some internal
upscaling. This can improve the OCR result significantly, especially for
low-resolution PDF scans.</p>

<p>Optimize for table recognition If set to true, the OCR logic makes sure that
the parsed text result is always returned line by line. This switch is
recommended for table OCR, receipt OCR, invoice processing and all other
type of input documents that have a table like structure.</p>

<p>Si quieres usar tu propia clave, también deberás especificar tu clave de
API.</p>

<p>You can get your own free API key by registering on <a href="https://ocr.space/ocrapi">OCR
space</a>  Here is a simple guide.  Find the link
"Register for free API key" Click on it and you will find a form to fill
in.  The form asks you to enter the following data Email Address First Name
Last Name How do you plan to use the OCR API? After filling it and
submit. You may also need to pass a captcha Then you will receive a
confirmation e-mail Find the link named "Yes, subscribe me to this list." in
that e-mail. Access that link and you will receive API key by e-mail soon.</p>

<h3>Microsoft Azure OCR</h3>

<p>Este motor utiliza la API del OCR de los Servicios Cognitivos de Visión
Artificial de Microsoft Azure.</p>

<p>It supports 24 languages including Chinese Simplified Chinese Traditional
Czech Danish Dutch English Finnish French German Greek Hungarian Italian
Japanese Korean Norwegian Polish Portuguese Russian Spanish Swedish Turkish
Arabic Romanian Serbian Cyrillic Serbian Latin Slovak</p>

<p>Estos son los ajustes de este motor:</p>

<p>Idioma: idioma de reconocimiento del texto. Por defecto detectar
automáticamente.</p>

<p>Detectar orientación de la imagen: si se activa, la API gira automáticamente
la imagen correctamente.</p>

<p>If you use your own key, you should get a subscription key for using
Microsoft Computer Vision API from the link below: Step 1: Create an
account.</p>

<p>https://azure.microsoft.com/en-ua/try/cognitive-services/</p>

<p>Ten en cuenta que debes crear una clave para la API de visión artificial. Es
el primer botón "Obtener clave de API" que encuentras con la navegación de
una sola letra. Actualmente Microsoft ofrece la opción de crear una clave de
prueba que funciona durante 7 días. También puedes crear una cuenta gratuita
de Azure para tener más pruebas. Al registrarse será obligatorio aportar los
datos de una tarjeta de crédito. Si ya tienes una cuenta con suscripción,
puedes saltarte este paso.</p>

<p>Step 2: Deploy Cognitive Services Now you have an azure account.  First
login on <a href="https://portal.azure.com">Azure Portal</a>  Wait until you get the
message Portal is Ready you are logged into azure portal.  Find the link
called All resources after All services button and activate it.  Wait until
you get the message Blade All resources are ready , your focus will be an
edit box, then press shift tab find a menu item called add and activate it.</p>

<p>Wait until you get the message Search the Marketplace, Type Cognitive
Services and press down arrow.  Wait until you get the message List of
options Cognitive Services one of five, then press enter.  Wait until you
get the message Blade Cognitive Services is ready press tab or b to find a
button named Create activate it.  Wait until you get the message Blade
Create is ready, your focus will be an edit box, type a name for this
resource. Note that Your resource name can only include alphanumeric
characters, ‘<em>,-’, and can’t end with ’</em>’ or ’-‘.  I choose NVDA_OCR.  Press
tab to go to Subscription combo box. Usually you can leave it as is.  Press
tab to go to Location combo box. Choose one close to your current
location. Be sure to remember this since location is required in engine
configuration.  Press tab to go to Pricing tie combo box. Usually a free tie
like F0 is adequate. If that is not enough you can choose other tier after
reading full pricing details in View full pricing details link.  Press tab
to go to Create new Resource group edit box. You should create one if you do
not have any Resource group. Press tab find Create new button.  Then press
tab go to Create Button to create this resource.  Wait until you get the
message Deployment succeeded.  Then find Go to resource button sometimes you
need go up to activate Notifications button before you can find Go to
resource button.  Wait until you get the message Blade Quick Start is busy.
Find the link named keys, then activate it.  Wait until you get the message
Blade Manage keys is ready.  Find edit box named key 1 or key 2. The content
of that edit box is the API key required in engine configuration. Press
Ctrl-C to copy it for engine configuration Then you can fill in these two
settings required if you use your own API key.  Azure resource Region: The
region you choose when deploying Cognitive Services in Azure Portal.  API
key: The key you get after successfully deploying Cognitive Services in
Azure Portal, KEY 2 is recommended.</p>

<h3>Baidu OCR</h3>

<p>This one is also a paid API with free quota provided by Baidu.  Baidu OCR
supports 10 languages including Chinese and English mixture English
Portuguese French German Italian Spanish Russian Japanese Korean This engine
can also get position of every character</p>

<p>Aquí están sus ajustes:</p>

<p>Obtener posición de cada carácter: permite operar de forma más precisa en
algunas aplicaciones inaccesibles. Al activar esta opción el reconocimiento
se volverá algo más lento.</p>

<p>Use Accurate API If is enabled will use a different endpoint.  That accurate
endpoint takes longer time but has higher quality and (If you use your own
API key its price is also higher).</p>

<p>Tiene cuatro conexiones con límites de cuota separados.</p>

<p>Basic OCR without any information about text location.  Currently 50000
times a day.  Basic OCR with information about text location.  Currently 500
times a day.  Accurate OCR without any information about text location.
Currently 500 times a day.  Accurate with information about text location.
Currently 50 times a day.</p>

<p>If you press the gesture which only read result, you are using endpoints
without any information about text location.  If you press the gesture which
shows an result viewer, you are using endpoints with information about text
location.</p>

<p>Aunque proporciona una cuota gratuita bastante generosa, su sitio web sólo
está en chino y no es muy accesible.</p>

<h3>Tencent AI OCR</h3>

<p>This API is free to use with frequency limit about two query per second.<br />
If you want to bypass the limit you can register your own API key. The website of this API is Chinese only and not quite accessible.
There is no information about language support in the document. According to my test Chinese and English and their mixture is supported.<br />
There is no additional configuration for this API.</p>

<h2>Registro de cambios</h2>

<h3>0.19</h3>

<p>Add Tencent Cloud OCR engine sponsored by <a href="https://intl.cloud.tencent.com">Tencent
Cloud</a> and <a href="http://www.siaa.org.cn">Aceessibility Research
Association</a>  Compatible with NVDA 2020.2 Removed
unavailable Sougou OCR and Machine Learning Engine by Oliver Edholm Fix
public endpoint on <a href="https://www.nvdacn.com">NVDA China Site</a></p>

<h3>0.18</h3>

<p>Compatible with python3 Introduce the concept of recognition source type and
engine type to reduce gesture usage.  Add a new unassigned gesture to cycle
through different recognition source types.  Add a new unassigned gesture to
cycle through different recognition engine types.  Add a new gesture to
recognize according to image source and engine type setting.  Add a new
unassigned gesture to show previous result in a virtual result document.</p>

<h3>0.17</h3>

<p>Fixed following issues: Jump directly to panel when switch to
onlineImageDescriber in settings dialog Fix wrong description in azure
analyzer</p>

<h3>0.16</h3>

<p>Se ha añadido un gesto para cancelar el reconocimiento</p>

<p>Fixed following issues: CheckListBox state change not announced Swap the
effect of repeated gesture not working in online image describer</p>

<h3>0.15</h3>

<p>Add an option to pop up a window containing message instead of speech or
braille message for text results Change checkboxes for visual features in
Microsoft Azure Image Analyzer into a CheckBoxList.</p>

<p>Fix following issues: Cannot load jpg image file from clipboard Result
document object do not show up after recognition.  Position in result
document objects are not reliable if image is resized internally.  Result
from Microsoft Azure Image Describer is in the same line which makes it hard
to navigate around.</p>

<h3>0.14</h3>

<p>Fixed some bugs: Cannot use your own API key in Microsoft Azure engines
Cannot get text result if there is a braille display</p>

<h3>0.13</h3>

<p>Ahora el complemento funciona al recargar los plugins sin reiniciar
(NVDA+ctrl+f3)</p>

<h3>0.12</h3>

<p>Fixed browse mode message of Microsoft Azure Image Describer The accent
color is now represented as NVDA colour descriptions.  Improved result
format of Microsoft Azure Image Analyser Improve document according to
review comments Fixed gesture inconsistency.  Control+Shift+NVDA for
clipboard while NVDA+ALT for navigator object Fix missing imageInfo error
while recognizing.</p>

<h3>0.11</h3>

<p>Added image description capability Change addon summary to online image
describer</p>

<h3>0.10</h3>

<p>Fix error using user's own API key in sougou API.  Fix unknown panel issue
by adding settings to supportedSettings</p>

<h3>0.9</h3>

<p>Fix double press gesture no effect issue.  Revised document to reflect
changes in code.  Clarified what kind of clipboard image is supported and
how to copy image for recognition.</p>

<p>Fixed the clipboard recognition cannot open result viewer issue.  Added
support to recognize copied local image file path in clipboard.</p>

<h3>0.8</h3>

<p>Added friendly notice if recognition result is empty.  Fixed another place
do not work well with non ascii config path</p>

<h3>0.6</h3>

<p>Added proxy settings for people with access of Internet behind a specific
proxy.  Added several general options.  Fix Unicode decode error due to
sending Unicode URL to urllib3.</p>

<h3>0.5</h3>

<p>Fix Unicode error if OCR engine upload image file directly instead of base64
encode.  Change gesture of recognizing clipboard to Control+Shift+NVDA+R
since NVDA+Shift+R is used in Word and Excel to define row headers in
tables, or to delete the definitions when pressed twice.</p>

<h3>0.4</h3>

<p>Corregido error de instalación cuando la ruta a la configuración contiene
caracteres no ASCII. Modificado un gesto para evitar colisiones con Golden
Cursor. Cambiado el motor por defecto a Microsoft Azure, ya que puede
detectar el idioma del texto automáticamente.</p>

<h3>0.3</h3>

<p>Add detail documentation on how to get API key of Microsoft Azure OCR Fix
issue about new installation.  Removed auto OCR since this feature is
problematic and may confuse with online engines. Auto OCR will be a separate
addon, when it is stable enough.</p>

